{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f3a7cc-f23a-43fd-8f29-050ddc6bf431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:16:22.221830Z",
     "iopub.status.busy": "2024-02-05T22:16:22.220239Z",
     "iopub.status.idle": "2024-02-05T22:16:22.263029Z",
     "shell.execute_reply": "2024-02-05T22:16:22.261791Z",
     "shell.execute_reply.started": "2024-02-05T22:16:22.221735Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e90f14-a776-47b2-9265-ff4fb8689608",
   "metadata": {},
   "source": [
    "> this notebook will follow the tutorial in:\n",
    "https://blog.gopenai.com/fine-tuning-dialogpt-medium-on-daily-dialog-dataset-a-step-by-step-guide-4eaecc1b9323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4453318c-3fa5-4b1e-86d1-af7ec9ef15a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:16:22.661381Z",
     "iopub.status.busy": "2024-02-05T22:16:22.659805Z",
     "iopub.status.idle": "2024-02-05T22:16:40.074640Z",
     "shell.execute_reply": "2024-02-05T22:16:40.073326Z",
     "shell.execute_reply.started": "2024-02-05T22:16:22.661302Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./env/lib/python3.10/site-packages (4.36.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./env/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./env/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./env/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed transformers-4.37.2\n",
      "Requirement already satisfied: datasets in ./env/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./env/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./env/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./env/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./env/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./env/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./env/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./env/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: multiprocess in ./env/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in ./env/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./env/lib/python3.10/site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./env/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./env/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: accelerate in ./env/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in ./env/lib/python3.10/site-packages (from accelerate) (0.20.2)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.10/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.10/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./env/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./env/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: triton==2.1.0 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./env/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install datasets\n",
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a8aa2-047a-43ae-bcde-56e23f48c6c9",
   "metadata": {},
   "source": [
    "# make my own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0a4b1-a071-446a-8fb9-2604542b6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/nlp-course/chapter5/5\n",
    "# https://huggingface.co/learn/nlp-course/chapter5/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5eb83a7-2330-46ad-a22f-a80cca669419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:30:40.460464Z",
     "iopub.status.busy": "2024-02-05T22:30:40.459240Z",
     "iopub.status.idle": "2024-02-05T22:30:40.493805Z",
     "shell.execute_reply": "2024-02-05T22:30:40.492289Z",
     "shell.execute_reply.started": "2024-02-05T22:30:40.460386Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92d28bf-d212-464e-a7c1-31d181ddf324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:30:40.614812Z",
     "iopub.status.busy": "2024-02-05T22:30:40.613583Z",
     "iopub.status.idle": "2024-02-05T22:30:40.657169Z",
     "shell.execute_reply": "2024-02-05T22:30:40.655402Z",
     "shell.execute_reply.started": "2024-02-05T22:30:40.614741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/DL103_2008.json',\n",
       " 'data/0288702916.json',\n",
       " 'data/DL320_2002.json',\n",
       " 'data/L65_2013.json',\n",
       " 'data/dlr7_2016-m.json',\n",
       " 'data/0331103315.json',\n",
       " 'data/DLR4_2012_A.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob('data/*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ef9a0-624d-4557-83b1-2d77bf194189",
   "metadata": {},
   "source": [
    "we can also set the splits\n",
    "\n",
    "`data_files = {\"train\": \"json_example.json\", \"test\": \"json_example.json\"}`\n",
    "`dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a66f020-e58e-4f87-8c59-54c361828133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:46:49.606624Z",
     "iopub.status.busy": "2024-02-05T22:46:49.606122Z",
     "iopub.status.idle": "2024-02-05T22:46:50.597805Z",
     "shell.execute_reply": "2024-02-05T22:46:50.596494Z",
     "shell.execute_reply.started": "2024-02-05T22:46:49.606584Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=glob('data/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ebcdf67-1352-46d6-a383-3fef1db122ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:46:51.805154Z",
     "iopub.status.busy": "2024-02-05T22:46:51.804584Z",
     "iopub.status.idle": "2024-02-05T22:46:51.868740Z",
     "shell.execute_reply": "2024-02-05T22:46:51.867092Z",
     "shell.execute_reply.started": "2024-02-05T22:46:51.805105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'page'],\n",
       "        num_rows: 106\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c149d2a5-a476-4cce-8c9a-114be87d2a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:31:35.649297Z",
     "iopub.status.busy": "2024-02-05T22:31:35.647730Z",
     "iopub.status.idle": "2024-02-05T22:31:35.694893Z",
     "shell.execute_reply": "2024-02-05T22:31:35.692759Z",
     "shell.execute_reply.started": "2024-02-05T22:31:35.649218Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['8 3765 A conformidade das máquinas continua a ser certificada pelo fabricante, sendo alargada a possibilidade de escolha de procedimentos de avaliação de conformidade para o caso das máquinas definidas no anexo IV em que se exigem procedimentos específicos',\n",
       "  'É ainda introduzido, no presente decreto -lei, um meca- nismo que permite a adopção de medidas específicas a nível comunitário, que exigem aos Estados membros a proibição ou a restrição da colocação no mercado de certos tipos de máquinas que apresentem os mesmos riscos para a saúde e a segurança das pessoas, quer devido a lacunas das normas har- monizadas pertinentes quer devido às suas características téc- nicas, ou submeter essas máquinas a condições especiais',\n",
       "  'Foram ouvidos os órgãos de governo próprio das Re- giões Autónomas',\n",
       "  'Foram ouvidas as associações representativas do sector',\n",
       "  'Assim: Nos termos da alínea a) do n.º 1 do artigo 198.º da Cons- tituição, o Governo decreta o seguinte: CAPÍTULO I Disposições gerais Artigo 1.º Objecto O presente decreto- lei estabelece as regras a que deve obedecer a colocação no mercado e a entrada em serviço das máquinas bem como a colocação no mercado das quase -máquinas, transpondo para a ordem jurídica interna a Directiva n.º 2006/42/CE, do Parlamento Europeu e do Conselho, de 17 de Maio, relativa às máquinas e que al- tera a Directiva n.º 95/16/CE, do Parlamento Europeu e do Conselho, de 29 de Junho, relativa à aproximação das legislações dos Estados membros respeitantes aos ascensores',\n",
       "  'Artigo 2.º Âmbito de aplicação 1 — As disposições do presente decreto -lei aplicam -se aos seguintes produtos: a) Máquinas; b) Equipamento intermutável; c) Componentes de segurança; d) Acessórios de elevação; e) Correntes, cabos e correias; f) Dispositivos amovíveis de transmissão mecânica; g) Quase -máquinas',\n",
       "  '2 — Excluem -se do âmbito do presente decreto -lei: a) Os componentes de segurança destinados a substi- tuir componentes idênticos, fornecidos pelo fabricante da máquina de origem; b) Os materiais específicos para feiras e ou parques de atracções; c) As máquinas especialmente concebidas ou colocadas em serviço para utilização nuclear cuja avaria possa causar uma emissão de radioactividade; d) As armas, incluindo as armas de fogo; e) Os seguintes meios de transporte: i) Tractores agrícolas e florestais para os riscos cobertos pelo Decreto- Lei n.º 74/2005, de 24 de Março, que aprova',\n",
       "  'Diário da República, 1.ª série — N.º 120 — 24 de Junho de 2008 MINISTÉRIO DA ECONOMIA E DA INOVAÇÃO Decreto-Lei n.º 103/2008 de 24 de Junho O Decreto -Lei n.º 320/2001, de 12 de Dezembro, proce- deu à codificação da legislação nacional que regulamenta a colocação no mercado e entrada em serviço das máquinas, à semelhança do que foi efectuado a nível comunitário com a Directiva n.º 98/37/CE, do Parlamento Europeu e do Conselho, de 17 de Maio, relativa à aproximação das legis- lações dos Estados membros respeitantes às máquinas',\n",
       "  'A Directiva n.º 98/37/CE será revogada, a partir de 29 de Dezembro de 2009, pela Directiva n.º 2006/42/CE, do Parlamento Europeu e do Conselho, de 17 de Maio, relativa às máquinas e que altera a Directiva n.º 95/16/CE, do Par- lamento Europeu e do Conselho, de 29 de Junho, relativa à aproximação das legislações dos Estados membros res- peitantes aos ascensores, transposta para a ordem jurídica interna pelo Decreto -Lei n.º 295/98, de 22 de Setembro',\n",
       "  'No que diz respeito à alteração efectuada à Directiva n.º 95/16/CE pela Directiva n.º 2006/42/CE, que tem por objectivo clarificar a fronteira de aplicação entre a Direc- tiva Máquinas e a Directiva Ascensores, a transposição será assegurada por diploma autónomo, tendo por base, na ordem jurídica interna, o Decreto -Lei n.º 295/98, de 22 de Setembro',\n",
       "  'A Directiva n.º 2006/42/CE também delimita de forma precisa a fronteira entre o seu âmbito de aplicação e o da Directiva n.º 2006/95/CE, do Conselho, de 12 de Dezem- bro, relativa à harmonização das legislações dos Estados membros no domínio do material eléctrico destinado a ser utilizado dentro de certos limites de tensão, transposta para a ordem jurídica interna pelo Decreto- Lei n.º 6/2008, de 10 de Janeiro. Esta delimitação encontra -se reflectida no presente decreto -lei',\n",
       "  'O presente decreto -lei tem, assim, como objectivo regu- lamentar a colocação no mercado e a entrada em serviço das máquinas, transpondo para o ordenamento jurídico interno a Directiva n.º 2006/42/CE na parte que respeita às máquinas',\n",
       "  'Pretende -se com o presente decreto -lei consolidar os resultados alcançados em termos de livre circulação e de segurança das máquinas e simultaneamente melhorar a aplicação da legislação vigente, definindo com maior pre- cisão o âmbito e os conceitos relativos à sua aplicação',\n",
       "  'O âmbito de aplicação é alargado e são clarificadas as fronteiras com os regimes constantes dos Decretos -Leis n.os 295/98, de 22 de Setembro, e 6/2008, de 10 de Ja- neiro',\n",
       "  'É também clarificada a gama de componentes de segu- rança que estão sujeitos ao cumprimento das disposições do presente decreto -lei, sendo incluída, em anexo, uma lista indicativa de componentes de segurança. É introduzido o conceito de quase -máquinas e estabelecidas regras para a sua colocação no mercado',\n",
       "  'Foi efectuado um aprofundamento dos requisitos essen- ciais de saúde e de segurança no sentido de melhorar a sua precisão, alargar a aplicação de alguns, que actualmente são apenas aplicáveis a máquinas móveis ou de elevação, a qualquer máquina que apresente os riscos em questão e incluir novos requisitos aplicáveis aos tipos de máquinas introduzidos no âmbito, sendo mantida a estrutura actual, nomeadamente a numeração, para minimizar o impacte nos utilizadores.'],\n",
       " 'page': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff05d794-de18-4674-83bb-44c8d10d3c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:46:57.082782Z",
     "iopub.status.busy": "2024-02-05T22:46:57.081940Z",
     "iopub.status.idle": "2024-02-05T22:46:57.159381Z",
     "shell.execute_reply": "2024-02-05T22:46:57.156987Z",
     "shell.execute_reply.started": "2024-02-05T22:46:57.082686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all utterances within a dialogue and map to 'dialog' key\n",
    "def concatenate_paragraphs(example):\n",
    "    example['page'] = \" \".join(example['text'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(concatenate_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb19276-8bd1-472d-b254-639882c4d365",
   "metadata": {},
   "source": [
    "> Note: not sure if this is really needed, but for simplicity will make a whole text per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55af3c70-e86f-4865-bc9b-49830c1d29d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:38:29.930593Z",
     "iopub.status.busy": "2024-02-05T22:38:29.929775Z",
     "iopub.status.idle": "2024-02-05T22:41:15.370586Z",
     "shell.execute_reply": "2024-02-05T22:41:15.369355Z",
     "shell.execute_reply.started": "2024-02-05T22:38:29.930515Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 51.5kB/s]\n",
      "vocab.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 2.48MB/s]\n",
      "merges.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.39MB/s]\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 642/642 [00:00<00:00, 1.72MB/s]\n",
      "pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 863M/863M [02:34<00:00, 5.57MB/s]\n",
      "/home/peterdays/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 398kB/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('microsoft/DialoGPT-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b777bdb8-7a90-4ac0-b985-6ce39d818ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:46:59.886534Z",
     "iopub.status.busy": "2024-02-05T22:46:59.884829Z",
     "iopub.status.idle": "2024-02-05T22:47:01.338403Z",
     "shell.execute_reply": "2024-02-05T22:47:01.337276Z",
     "shell.execute_reply.started": "2024-02-05T22:46:59.886454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the dataset\n",
    "# https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "def encode(examples):\n",
    "    encoded = tokenizer(examples['page'], truncation=True, padding='max_length', max_length=128)\n",
    "    encoded['labels'] = encoded['input_ids'][:]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "encoded_dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4190f6-3656-4781-a05f-a0fc5d401ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:46:09.102739Z",
     "iopub.status.busy": "2024-02-05T22:46:09.102005Z",
     "iopub.status.idle": "2024-02-05T22:46:09.143052Z",
     "shell.execute_reply": "2024-02-05T22:46:09.141608Z",
     "shell.execute_reply.started": "2024-02-05T22:46:09.102656Z"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faa65fdb-9802-4f32-a248-8c1d82bf8720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:47:32.062728Z",
     "iopub.status.busy": "2024-02-05T22:47:32.061822Z",
     "iopub.status.idle": "2024-02-05T22:47:33.025762Z",
     "shell.execute_reply": "2024-02-05T22:47:33.025016Z",
     "shell.execute_reply.started": "2024-02-05T22:47:32.062648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=tempfile.mkdtemp(),   # output directory\n",
    "    num_train_epochs=10,             # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=None,                # directory for storing logs\n",
    "    fp16=True                        # use floating point 16 bit precision for training\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de615ecb-e60c-4388-808e-a672b0305fea",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77244853-6b0e-4ef9-9df9-b1b54f33bc39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T22:47:59.999254Z",
     "iopub.status.busy": "2024-02-05T22:47:59.998477Z",
     "iopub.status.idle": "2024-02-05T22:48:06.015379Z",
     "shell.execute_reply": "2024-02-05T22:48:06.014232Z",
     "shell.execute_reply.started": "2024-02-05T22:47:59.999183Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacty of 5.79 GiB of which 65.88 MiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 5.46 GiB is allocated by PyTorch, and 155.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate before fine-tuning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pre_eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get predictions for validation set before fine tuning for 10 samples\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pre_val_predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(encoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)))\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/transformers/trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3094\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3095\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3099\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3105\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/transformers/trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3283\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3284\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3285\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3286\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/transformers/trainer.py:3501\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3500\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3501\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3502\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/transformers/trainer.py:2795\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2794\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/accelerate/utils/operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/accelerate/utils/operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1107\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1107\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1110\u001b[0m     output \u001b[38;5;241m=\u001b[39m (lm_logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal/Volupal/ElevaQ/env/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacty of 5.79 GiB of which 65.88 MiB is free. Including non-PyTorch memory, this process has 5.71 GiB memory in use. Of the allocated memory 5.46 GiB is allocated by PyTorch, and 155.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Evaluate before fine-tuning\n",
    "pre_eval_results = trainer.evaluate(encoded_dataset['train'])\n",
    "\n",
    "# Get predictions for validation set before fine tuning for 10 samples\n",
    "pre_val_predictions = trainer.predict(encoded_dataset['train'].select(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be38ba7-055a-4e48-8c42-93f992fc50a9",
   "metadata": {},
   "source": [
    "TO DO: Add the gpu to this model!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3a389-9844-4871-ad52-bb90b5a2115c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
